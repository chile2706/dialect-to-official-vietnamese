# -*- coding: utf-8 -*-
"""finetune_phowhisper_fix1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BcV9edKhVCw7kkPyt0_hV066XUDcQqFx
"""

# from google.colab import drive
# drive.mount('/content/drive')

# import os
# os.chdir("/content/drive/MyDrive/CSCI_5541_project")

# print(os.getcwd())  # Now it will correctly show the new path

# !pip install datasets

# Step 4: Now import torch freshly from your local user install
import torch
import torchaudio
import pandas as pd
from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq, pipeline
from datasets import load_dataset, Audio
import pickle
import numpy as np

print("Everything imported successfully!")

import pandas as pd

# Load all your parquet files
df = pd.read_parquet("nghe_an.parquet")
# df_hanoi = pd.read_parquet("hanoi.parquet")
# df_hue = pd.read_parquet("hue.parquet")
# df_saigon = pd.read_parquet("saigon.parquet")

# # Combine them
# df = pd.concat([df_nghe_an, df_hanoi, df_hue, df_saigon], ignore_index=True)

# # Done! Now check
print(df.shape)
print(df.head())

# df.to_parquet("combined_provinces.parquet")

print(df["province_name"].unique())
print(df.shape)
print(df['audio'][0])

import torchaudio
import torch
import numpy as np

def resample_audio(df, target_sampling_rate=16000):
    print("Resampling audio to 16kHz...")

    for i in range(len(df)):
        original_waveform = np.array(df.loc[i, "audio"]["array"])
        original_waveform = torch.from_numpy(original_waveform).float()

        original_sampling_rate = df.loc[i, "audio"]["sampling_rate"]

        # Only resample if different from target
        if original_sampling_rate != target_sampling_rate:
            resampler = torchaudio.transforms.Resample(orig_freq=original_sampling_rate, new_freq=target_sampling_rate)
            resampled_waveform = resampler(original_waveform)

            # Update
            df.at[i, "audio"]["array"] = resampled_waveform.numpy()
            df.at[i, "audio"]["sampling_rate"] = target_sampling_rate

    print("Resampling complete!")
    return df

df = resample_audio(df)

from sklearn.model_selection import train_test_split

# First split: train and temp (val + test)
train_df, temp_df = train_test_split(
    df,
    test_size=0.1,
    random_state=42,
    # stratify=df["province_name"]  # <<< stratify by province_name
)

# train_df, test_df = train_test_split(
#     df,
#     test_size=0.2,
#     random_state=42,
#     stratify=df["province_name"]  # <<< stratify by province_name
# )

# Second split: validation and test
val_df, test_df = train_test_split(
    temp_df,
    test_size=0.5,
    random_state=42,
    # stratify=temp_df["province_name"]  # <<< stratify by province_name again
)

print(f"Train size: {len(train_df)}")
print(f"Val size: {len(val_df)}")
print(f"Test size: {len(test_df)}")

from torch.utils.data import Dataset

class WhisperDataset(Dataset):
    def __init__(self, df, processor):
        self.df = df
        self.processor = processor

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        audio = self.df.iloc[idx]["audio"]["array"]
        sampling_rate = self.df.iloc[idx]["audio"]["sampling_rate"]
        text = self.df.iloc[idx]["text"]
        province_name = self.df.iloc[idx]["province_name"]  # <-- use province_name here

        inputs = self.processor(audio, sampling_rate=sampling_rate, return_tensors="pt")
        labels = self.processor.tokenizer(text, return_tensors="pt")

        input_values = inputs.input_features.squeeze(0)
        labels = labels.input_ids.squeeze(0)

        return {
            "input_features": input_values,
            "labels": labels,
            "province_name": province_name  # <-- return province_name
        }

from dataclasses import dataclass
from typing import Any, List, Dict, Union
import torch

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any
    padding: Union[bool, str] = True

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:
        input_features = [{"input_features": feature["input_features"]} for feature in features]
        label_features = [{"input_ids": feature["labels"]} for feature in features]
        regions = [feature["province_name"] for feature in features]  # <-- Collect province_name manually

        batch = self.processor.feature_extractor.pad(
            input_features,
            padding=self.padding,
            return_tensors="pt"
        )

        labels_batch = self.processor.tokenizer.pad(
            label_features,
            padding=self.padding,
            return_tensors="pt"
        )

        labels = labels_batch["input_ids"].masked_fill(labels_batch["input_ids"] == self.processor.tokenizer.pad_token_id, -100)

        batch["labels"] = labels
        batch["province_name"] = regions  # <-- Add province_name to the batch

        return batch

from transformers import AutoProcessor

# Load the PhoWhisper processor
processor = AutoProcessor.from_pretrained("vinai/PhoWhisper-large")

data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)

# !pip install evaluate
# !pip install jiwer
# !pip install bert_score

import evaluate

# Load evaluation metrics
wer_metric = evaluate.load("wer")
bert_score_metric = evaluate.load("bertscore")

# 4. Define metric functions
def compute_metrics(predictions, references):
    wer = wer_metric.compute(predictions=predictions, references=references)
    bertscore = bert_score_metric.compute(predictions=predictions, references=references, lang="vi")
    return {
        "wer": wer,
        "bertscore_precision": sum(bertscore["precision"]) / len(bertscore["precision"]),
        "bertscore_recall": sum(bertscore["recall"]) / len(bertscore["recall"]),
        "bertscore_f1": sum(bertscore["f1"]) / len(bertscore["f1"]),
    }

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# device = torch.device("cpu")
print("Using device:", device)

from transformers import AutoModelForSpeechSeq2Seq

# Load the PhoWhisper model
model = AutoModelForSpeechSeq2Seq.from_pretrained(
    "vinai/PhoWhisper-large",
)
model = model.to(device)

from torch.utils.data import DataLoader

# Create datasets
train_dataset = WhisperDataset(train_df, processor)
val_dataset = WhisperDataset(val_df, processor)
test_dataset = WhisperDataset(test_df, processor)

# 4. Define Dataloaders
train_loader = DataLoader(train_dataset, batch_size=12, shuffle=True, collate_fn=data_collator)
val_loader = DataLoader(val_dataset, batch_size=12, shuffle=False, collate_fn=data_collator)
test_loader = DataLoader(test_dataset, batch_size=12, shuffle=False, collate_fn=data_collator)

import torch
from torch.optim import AdamW
from torch.optim import Adam
from torch.optim import SGD
from tqdm import tqdm

import matplotlib.pyplot as plt



# def train_model(model, train_loader, val_loader, optimizer, num_epochs=5):
#     print("Starting manual training...")

#     best_wer = float("inf")
#     best_model_path = None

#     # Track metrics
#     train_losses = []
#     val_wers = []
#     val_bertscores = []

#     for epoch in range(num_epochs):
#         model.train()
#         running_loss = 0.0
#         progress_bar = tqdm(train_loader, desc=f"Training Epoch {epoch+1}")

#         for batch in progress_bar:
#             input_features = batch["input_features"].to(device)
#             labels = batch["labels"].to(device)

#             optimizer.zero_grad()
#             outputs = model(input_features=input_features, labels=labels)
#             loss = outputs.loss

#             loss.backward()
#             optimizer.step()

#             running_loss += loss.item()
#             progress_bar.set_postfix({"loss": loss.item()})

#         avg_train_loss = running_loss / len(train_loader)
#         train_losses.append(avg_train_loss)
#         print(f"Epoch {epoch+1} Average Training Loss: {avg_train_loss:.4f}")

#         # Evaluate
#         metrics = evaluate_model(model, val_loader)
#         val_wers.append(metrics["wer"])
#         val_bertscores.append(metrics["bertscore_f1"])

#         # Save checkpoint
#         save_path = f"./pho_whisper_finetuned/model_epoch_{epoch+1}.pt"
#         torch.save(model.state_dict(), save_path)
#         print(f"Saved model checkpoint: {save_path}")

#         if metrics["wer"] < best_wer:
#             best_wer = metrics["wer"]
#             best_model_path = save_path
#             print(f"New best model found at {save_path} with WER {best_wer:.4f}")

#     # Load best model
#     if best_model_path:
#         print(f"\nLoading best model from {best_model_path} with WER {best_wer:.4f}")
#         model.load_state_dict(torch.load(best_model_path))

#     # Save final model
#     torch.save(model.state_dict(), "./pho_whisper_finetuned/final_model.pt")
#     print("Training complete! Final model saved.")

#     # Plot metrics
#     epochs = list(range(1, num_epochs + 1))

#     plt.figure(figsize=(15, 4))

#     plt.subplot(1, 3, 1)
#     plt.plot(epochs, train_losses, label="Train Loss")
#     plt.title("Training Loss")
#     plt.xlabel("Epoch")
#     plt.ylabel("Loss")

#     plt.subplot(1, 3, 2)
#     plt.plot(epochs, val_wers, label="Validation WER", color="orange")
#     plt.title("Validation WER")
#     plt.xlabel("Epoch")
#     plt.ylabel("WER")

#     plt.subplot(1, 3, 3)
#     plt.plot(epochs, val_bertscores, label="BERTScore F1", color="green")
#     plt.title("Validation BERTScore F1")
#     plt.xlabel("Epoch")
#     plt.ylabel("F1 Score")

#     plt.tight_layout()
#     learning_rate = optimizer.param_groups[0]['lr']
#     plt.savefig(f"metrics_plot_{optimizer.__class__.__name__}_lr{learning_rate}.png") # Save to file

#   # 6. Define evaluation function
# def evaluate_model(model, val_loader):
#     print("Evaluating on validation set...")
#     model.eval()

#     decoded_preds = []
#     decoded_labels = []

#     with torch.no_grad():
#         for batch in tqdm(val_loader, desc="Evaluating"):
#             input_features = batch["input_features"].to(device)
#             labels = batch["labels"].to(device)

#             outputs = model.generate(input_features=input_features, max_length=225)
#             pred_texts = processor.batch_decode(outputs, skip_special_tokens=True)
#             label_texts = processor.batch_decode(labels, skip_special_tokens=True)

#             decoded_preds.extend(pred_texts)
#             decoded_labels.extend(label_texts)

#     # Compute both WER and BERTScore
#     metrics = compute_metrics(decoded_preds, decoded_labels)
#     print(f"Validation WER: {metrics['wer']:.4f}")
#     print(f"Validation BERTScore F1: {metrics['bertscore_f1']:.4f}")

#     return metrics

def train_model(model, train_loader, val_loader, optimizer, num_epochs=5):
    print("Starting manual training...")

    best_wer = float("inf")
    best_model_path = None

    # Track metrics
    train_losses = []
    val_wers = []
    val_bertscores = []

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        progress_bar = tqdm(train_loader, desc=f"Training Epoch {epoch+1}")

        for batch in progress_bar:
            input_features = batch["input_features"].to(device)
            labels = batch["labels"].to(device)

            optimizer.zero_grad()
            outputs = model(input_features=input_features, labels=labels)
            loss = outputs.loss

            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            progress_bar.set_postfix({"loss": loss.item()})

        avg_train_loss = running_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        print(f"Epoch {epoch+1} Average Training Loss: {avg_train_loss:.4f}")

        # Evaluate
        metrics = evaluate_model(model, val_loader)
        val_wers.append(metrics["wer"])
        val_bertscores.append(metrics["bertscore_f1"])

        # Save checkpoint
        save_path = f"./pho_whisper_finetuned/model_epoch_{epoch+1}.pt"
        torch.save(model.state_dict(), save_path)
        print(f"Saved model checkpoint: {save_path}")

        if metrics["wer"] < best_wer:
            best_wer = metrics["wer"]
            best_model_path = save_path
            print(f"New best model found at {save_path} with WER {best_wer:.4f}")

    # Load best model
    if best_model_path:
        print(f"\nLoading best model from {best_model_path} with WER {best_wer:.4f}")
        model.load_state_dict(torch.load(best_model_path))

    # Save final model
    optimizer_name = optimizer.__class__.__name__
    learning_rate = optimizer.param_groups[0]['lr']
    model_filename = f"./pho_whisper_finetuned/final_model_{optimizer_name}_lr{learning_rate}.pt"
    torch.save(model.state_dict(), model_filename)
    # torch.save(model.state_dict(), "./pho_whisper_finetuned/final_model.pt")
    print("Training complete! Final model saved.")

    # Plot metrics
    epochs = list(range(1, num_epochs + 1))

    plt.figure(figsize=(15, 4))

    plt.subplot(1, 3, 1)
    plt.plot(epochs, train_losses, label="Train Loss")
    plt.title("Training Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")

    plt.subplot(1, 3, 2)
    plt.plot(epochs, val_wers, label="Validation WER", color="orange")
    plt.title("Validation WER")
    plt.xlabel("Epoch")
    plt.ylabel("WER")

    plt.subplot(1, 3, 3)
    plt.plot(epochs, val_bertscores, label="BERTScore F1", color="green")
    plt.title("Validation BERTScore F1")
    plt.xlabel("Epoch")
    plt.ylabel("F1 Score")

    plt.tight_layout()
    learning_rate = optimizer.param_groups[0]['lr']
    plt.savefig(f"./plot_output/metrics_plot_{optimizer.__class__.__name__}_lr{learning_rate}.png") # Save to file
    plt.show()

  # 6. Define evaluation function
def evaluate_model(model, val_loader):
    print("Evaluating on validation set...")
    model.eval()

    decoded_preds = []
    decoded_labels = []

    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Evaluating"):
            input_features = batch["input_features"].to(device)
            labels = batch["labels"].to(device)

            outputs = model.generate(input_features=input_features, max_length=225)
            pred_texts = processor.batch_decode(outputs, skip_special_tokens=True)
            label_texts = processor.batch_decode(labels, skip_special_tokens=True)

            decoded_preds.extend(pred_texts)
            decoded_labels.extend(label_texts)

    # Compute both WER and BERTScore
    metrics = compute_metrics(decoded_preds, decoded_labels)
    print(f"Validation WER: {metrics['wer']:.4f}")
    print(f"Validation BERTScore F1: {metrics['bertscore_f1']:.4f}")

    return metrics


# 7. Define test function
def test_model(model, test_loader, output_csv_path="test_predictions.csv"):
    print("Running predictions on test set...")
    model.eval()

    decoded_preds = []
    decoded_labels = []
    province_names = []  # <-- Rename from regions to province_names

    with torch.no_grad():
        for batch in tqdm(test_loader, desc="Testing"):
            input_features = batch["input_features"].to(device)
            labels = batch["labels"].to(device)
            batch_province_names = batch["province_name"]

            outputs = model.generate(input_features=input_features, max_length=225)
            pred_texts = processor.batch_decode(outputs, skip_special_tokens=True)
            label_texts = processor.batch_decode(labels, skip_special_tokens=True)

            decoded_preds.extend(pred_texts)
            decoded_labels.extend(label_texts)
            province_names.extend(batch_province_names)  # <-- Use province_names

    # Save CSV with province_name, reference text, prediction text
    results_df = pd.DataFrame({
        "province_name": province_names,  # <-- Here too
        "reference_text": decoded_labels,
        "prediction_text": decoded_preds,
    })
    results_df.to_csv(output_csv_path, index=False)
    print(f"Test predictions saved to {output_csv_path}")

    # Compute overall metrics
    overall_metrics = compute_metrics(decoded_preds, decoded_labels)
    print(f"Overall Test WER: {overall_metrics['wer']:.4f}")
    print(f"Overall Test BERTScore F1: {overall_metrics['bertscore_f1']:.4f}")

    # Compute metrics **per province_name**
    print("\nPer-province metrics:")
    for province_name in sorted(set(province_names)):
        province_preds = [p for p, prov in zip(decoded_preds, province_names) if prov == province_name]
        province_labels = [l for l, prov in zip(decoded_labels, province_names) if prov == province_name]

        province_metrics = compute_metrics(province_preds, province_labels)

        print(f"- {province_name}:")
        print(f"  WER: {province_metrics['wer']:.4f}")
        print(f"  BERTScore F1: {province_metrics['bertscore_f1']:.4f}")

    return results_df, overall_metrics


# 8. Train and Test
# Call it:
# results_df, test_metrics = test_model(model, test_loader, output_csv_path="test_predictions_pretrain.csv")

# Define optimizer
optimizer = SGD(model.parameters(), lr=1e-6)

# Train model (saves best and final model inside)
train_model(model, train_loader, val_loader, optimizer, num_epochs=5)

# Load final model (same naming as inside training function)
optimizer_name = optimizer.__class__.__name__
learning_rate = optimizer.param_groups[0]['lr']
model_filename = f"./pho_whisper_finetuned/final_model_{optimizer_name}_lr{learning_rate}.pt"
model.load_state_dict(torch.load(model_filename))

# Evaluate and export test results
csv_output_path = f"./csv_file_output/test_predictions_{optimizer_name}_lr{learning_rate}.csv"
results_df, test_metrics = test_model(model, test_loader, output_csv_path=csv_output_path)
